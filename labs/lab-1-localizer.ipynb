{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba49356-0ab2-47c3-be34-abaea8dacdf0",
   "metadata": {},
   "source": [
    "# Lab 1: Localizing language in the brain\n",
    "In this notebook, we work through two different approaches for localizing language (or speech) processing in the brain. We'll use a general linear model (GLM) to analyze an experimental dataset contrasting sentences with a control condition of nonsense words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db426618-9e17-42ff-8a5f-08fdb788bede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some useful Python modules\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd53e6-b352-4c35-9ecd-477d9f5a220d",
   "metadata": {},
   "source": [
    "### Part 1: Language localizer \"by hand\"\n",
    "In this section, we'll work through the basics of handling fMRI data and the experimental design, then run a very simple linear regression model to identify voxels that respond more strongly to sentences than the control condition. We'll code several steps of the analysis \"by hand\" so that you can see what's happening under the hood; for production-level analyses, analysis software will typically abstract much of the nitty-gritty (see Part 2). This \"localizer\" experiment for identifying language areas is intended to provide a very simple example following the approach popularized by Fedorenko and colleagues (e.g., [2010](https://doi.org/10.1152/jn.00032.2010), [2024](https://doi.org/10.1038/s41583-024-00802-4))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e448193-8d6c-4652-bc54-6ae7a6a5f7db",
   "metadata": {},
   "source": [
    "For Part 1, we'll use an open dataset of $N=10$ subjects performing a classical language localizer task graciously provided by Christophe Pallier as part of the [*Nilearn*](https://nilearn.github.io/) package ([Abraham et al., 2014](https://doi.org/10.3389/fninf.2014.00014)). In this experiment, subjects covertly read either (1) meaningful sentences composed of a series words (in French) presented rapidly one-by-one at the center of the screen (labeled `language`, or (2) meaningless sequences of nonwords comprising strings of consonants presented in the same way (labeled `string`) ([Pallier, 2019](https://osf.io/k4jp8)) We'll use *Nilearn*'s `fetch_language_localizer_demo_dataset` convenience function to download the dataset. Set `data_dir` to a path on your computer where you'd like the demo data to live (otherwise, *Nilearn* will download the dataset into your home directory). It may take a couple minutes to download the dataset depending on your connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3a12b-680c-4b27-92cd-a13330577ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in dataset using nilearn\n",
    "from nilearn import datasets\n",
    "\n",
    "# Change this path to match your local machine\n",
    "data_dir = '/Users/snastase/Work/psyc599/nilearn-data'\n",
    "dataset = datasets.fetch_language_localizer_demo_dataset(data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00c3f9a-0b02-4f18-a944-d73a36d2e00b",
   "metadata": {},
   "source": [
    "Have a look at what file paths are included in the `datasets` object to orient yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc4eccd-5b96-4376-b783-4e5d438ad7e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore dataset:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56573b6e-7cbf-47dc-9fec-71360a7736cd",
   "metadata": {},
   "source": [
    "Next, let's pick an examples subject (e.g., `sub-01`) for prototyping. We'll set up a couple paths so that we can more easily load in (1) the `_bold.nii.gz` file (the fMRI data), (2) the `_events.tsv` file (the experimental design), and (3) `desc-confounds_regressors.tsv` file (head motion) for this subject. We'll ignore the confound regressors for right now to keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a86fe6-ab03-4a1e-a758-af94264858c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick example subject, e.g., sub-01 or sub-02\n",
    "subject = 'sub-01'\n",
    "\n",
    "# Set up files for BOLD data, events, and confounds\n",
    "bids_dir = join(data_dir, 'fMRI-language-localizer-demo-dataset')\n",
    "deriv_dir = join(bids_dir, 'derivatives')\n",
    "\n",
    "bold_f = join(deriv_dir, subject, 'func', f'{subject}_task-languagelocalizer_desc-preproc_bold.nii.gz')\n",
    "events_f = join(bids_dir, subject, 'func', f'{subject}_task-languagelocalizer_events.tsv')\n",
    "confounds_f = join(deriv_dir, subject, 'func', f'{subject}_task-languagelocalizer_desc-confounds_regressors.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a073bb-e96b-4856-9b88-22843248bbd8",
   "metadata": {},
   "source": [
    "Let's also establish a couple basic parameters for this experiment. There are $229~\\mathrm{TRs}$ (time points) in this fMRI dataset and $\\mathrm{TR} = 1.5~\\mathrm{s}$. (These parameters were set by the experimenters prior to data collection and can be found in the `_bold.json` file accompanying each `_bold.nii.gz` file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e364a3-1c7c-4abd-830b-7c0adae4578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fMRI metadata for this experiment\n",
    "n_trs = 229\n",
    "tr = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d75a1-0abb-46d9-83cb-4d2e9a3f9ccb",
   "metadata": {},
   "source": [
    "#### Preparing the fMRI data for modeling\n",
    "Use `nib.load` (from the *NiBabel* module you imported above) to load in the fMRI data for this subject (`bold_f`). Inspect the `.shape` of the BOLD image and use `assert` to check the number of TRs defined above (`n_trs`) against the BOLD image itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed4b7f9-f3e0-4d44-be33-e5c51e9ef2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BOLD data and check shape:\n",
    "\n",
    "# Check that number of TRs matches:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2cf310-5fe4-4684-bb2b-25562589fa1a",
   "metadata": {},
   "source": [
    "Next, use `index_img` from `nilearn.image` (you'll need to import it) to extract the first time point (TR) of the BOLD dataset. Use `plot_epi` from `nilearn.plotting` to visualize the first TR of the BOLD dataset. Note that the acronyms *blood-oxygenation-level–dependent* (BOLD), *echo-planar imaging* (EPI), and *functional magnetic resonance imaging* (fMRI) are all being used fairly interchangeably here to refer to the functional time series of fMRI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df93831c-f333-42e6-98c5-7eaf9cc112ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot first time point of BOLD image:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666fed95-7325-493b-90ae-508bec9f3de9",
   "metadata": {},
   "source": [
    "The BOLD dataset is a roughly-cubical 3-dimensional image \"brick\" acquired at each time point, but the brain itself is a round little blob inside that brick. This means that BOLD images also include a lot of \"empty\" voxels that are completely outside the brain and contain no meaningful signal (look at all those black voxels outside the edges of the brain). How can we zoom in on the brain itself? Although everybody's brain is unique in shape and size, these data have already been spatially normalized to match the widely used MNI template; that is, this subject's brain has already been standardized to a common size/shape in a common coordinate system called [MNI space](https://www.lead-dbs.org/about-the-mni-spaces/). This means we can load in a standardized \"mask\" for MNI space and use this mask to isolate the brain voxels in our BOLD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65382cee-ee5a-435d-9336-7b79869cfbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in MNI template brain mask\n",
    "mni_mask = datasets.load_mni152_brain_mask()\n",
    "print(mni_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e8446a-880f-41ea-9b4e-308c491fc368",
   "metadata": {},
   "source": [
    "Note that the MNI mask appears to have many more voxels in all three dimensions than our BOLD dataset. This is because the MNI template was acquired at a much higher resolution ($1~\\mathrm{mm^3}$) than our BOLD dataset ($4.5~\\mathrm{mm^3}$) (and may also be on a different \"grid\" overall). Use `resample_to_img` from `nilearn.image` to resample the mask (`source_img`) to the same resolution and grid as our BOLD data (`target_img`). Check (e.g., using `assert`) that the mask shape now matches the spatial dimensions of the BOLD dataset. In my code, I call this resampled MNI mask `mni_mask_r`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22687a9a-ec7b-4ed6-ae52-7fdac0be245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample MNI mask image to match BOLD image:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595454df-ea59-4e02-ae5b-1a821ebbb460",
   "metadata": {},
   "source": [
    "Use `plot_roi` from `nilearn.plotting` to plot the resampled MNI mask on top of (i.e., `bg_img` in `plot_roi`) the first time point of the BOLD dataset—to prove to yourself that they're reasonably well-aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1940b4c-a249-4bb9-88d0-64752adc6eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mask on top of first TR of BOLD image:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1076fe2f-d52a-423b-aadf-7819e9ff63d9",
   "metadata": {},
   "source": [
    "To actually apply the mask to the BOLD data, we'll use `NiftiMasker` from `nilearn.maskers`. Keeping track of voxel coordinates for going back and forth between an original image and the masked image is pretty annoying—using `NiftiMasker` here will save us a lot of pain and suffering. We'll first initalize a `masker` object using the resampled MNI mask we previously created. (I called my resampled mask `mni_mask_r`, but you'll need to change the input to match your own variable name.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc018aa-1c6d-40ad-8c72-6ebc05ee7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creater a \"masker\" based on the resampled mask\n",
    "from nilearn.maskers import NiftiMasker\n",
    "masker = NiftiMasker(mask_img=mni_mask_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ca53b-ed4d-41c0-a93b-6a117ee42a3e",
   "metadata": {},
   "source": [
    "Now, use the `masker`'s `.fit_transform` method to apply the mask to your BOLD dataset. Have a look at the new shape/axes of the dataset. How many voxels do we have left after masking for only brain voxels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1252ea3-32c1-4657-a1fb-7adda0a7bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask the BOLD data using the MNI mask:\n",
    "# (you can safely ignore the warning here)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d7b80a-e2ad-4b4a-b191-f6b9d84d7df5",
   "metadata": {},
   "source": [
    "Finally, use `zscore` from `scipy.stats` to z-score the time series for each voxel. You'll need to use the `axis` argument in `zscore` to get this right. Name this final format of the BOLD dataset `Y`. This will be the target variable in our regression equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d6b8c-3a50-40bc-82aa-0021888680dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-score voxelwise time series:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993b343-fe9a-47ee-aec8-b4ecddd07660",
   "metadata": {},
   "source": [
    "#### Preparing the design matrix\n",
    "Now, we'll read in the events and confounds files (tab-separated tables) as *pandas* *DataFrame*s. Have a look at these tables: the `events` table contains the onsets and durations of the two experimental conditions over the course of our subject's scan; the `confounds` table contains six (three rotation, three translation) time series capturing their head motion over the course of the scan. We'll focus on just the `events` for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb61685-b6e8-4344-87de-28c7e12b9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in events and confounds tables\n",
    "events = pd.read_csv(events_f, delimiter='\\t')\n",
    "confounds = pd.read_csv(confounds_f, delimiter='\\t')\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77cdaa8-71ee-4715-8983-7de913093cb7",
   "metadata": {},
   "source": [
    "We need to convert the onsets, durations, and condition labels into a design matrix with columns of $0$s and $1$s marking the occurrence of each condition for each time point. Here's a little loop to do this for you—make sure you understand what it's doing. I create a time series called `language_events` with $1$s indicating where the `language` task (meaningful sentences) is occuring (and $0$s elswhere), and a time series called `control_events` with $1$s where the `control` task (meaningless strings) is occurring (and $0$s elsewhere). Note that I'm using `np.round` to implicitly downsample the onsets and durations to the sampling rate (i.e., TR) of the fMRI data. (Production-level code will handle this in a more sophisticated way.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e048d4-837f-4f07-bd36-5ff31043495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with empty time series \n",
    "language_events = np.zeros((n_trs))\n",
    "control_events = np.zeros((n_trs))    \n",
    "\n",
    "# Add 1s to mark language/control events\n",
    "for event in events.itertuples():\n",
    "    onset = int(np.round(event.onset / tr))\n",
    "    duration = int(np.round(event.duration / tr))\n",
    "    if event.trial_type == 'language':\n",
    "        language_events[onset : onset + duration] = 1\n",
    "    elif event.trial_type == 'string':\n",
    "        control_events[onset : onset + duration] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3e2d39-32b3-436c-b118-5662a47b4a92",
   "metadata": {},
   "source": [
    "Use `plt.plot` to create line plots of the time series for the two conditions. You may want to use `plt.subplots` to create two adjacent plots with shared axes and adjust the `figsize`; e.g., for a wide figure, use `figsize=(8, 2)`. Make sure to correctly label the time axis units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f545b-7661-4802-97a4-650dfd91c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot language and control time series:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38561664-3fdd-45ba-a047-6222006be187",
   "metadata": {},
   "source": [
    "The above plot reflects the hypothesized \"neural\" activity for the two different experimental conditions—but with fMRI, we can't directly measure the temporally-precise neural activity; instead, we measure the hemodynamic response (i.e., the BOLD response), which is sluggish and smooth in time. To update our model to better reflect the BOLD signal, we will load in a predefined hemodynamic response function (HRF) ([Glover, 1999](https://doi.org/10.1006/nimg.1998.0419)). Let's visualize the HRF first, before we apply it to our regressors. Note that the HRF is matched to our TR and peaks at $\\approx5~\\mathrm{s}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4552127a-216f-41ff-8ec0-65ecd268e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hemodynamic response function (HRF)\n",
    "from nilearn.glm.first_level import glover_hrf\n",
    "hrf = glover_hrf(tr, oversampling=1, time_length=30)\n",
    "\n",
    "plt.plot(np.arange(0, 30, 1.5), hrf, c='darkred')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59eee95-8bc4-47f0-85f3-a2ae6f46797c",
   "metadata": {},
   "source": [
    "Next, we'll convolve this HRF with the actual time series (I'll provde the code for this to avoid any confusion). You can think of this [convolution](https://en.wikipedia.org/wiki/Convolution) operation as effectively sliding the two time series alongside each other and summing the products of their overlap. Use `plt.plot` to visualize the two time series after convolution with an HRF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bdf8eb-3240-4932-9f30-d83f78a215cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolve boxcars with HRF and plot\n",
    "language_regressor = np.convolve(language_events, hrf)[:n_trs]\n",
    "control_regressor = np.convolve(control_events, hrf)[:n_trs]\n",
    "\n",
    "# Plot time series after convolution with HRF:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337ec7d-e5b2-4498-8b39-50ec3798fd85",
   "metadata": {},
   "source": [
    "Our model is almost ready. Lastly, we'll stack these two time series as columns in a matrix alongside a column of $1$s to serve as the intercept. We'll call this our design matrix `X`. Use `matshow` or `imshow` to plot the design matrix with a grayscale colormap (e.g., `binary_r`). (If you use `matshow`, you may want to use, e.g., `np.repeat` to artificially widen the matrix for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3cb07-ff04-45c0-901a-4420cc533648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create design matrix\n",
    "X = np.column_stack((np.ones(n_trs),\n",
    "                     language_regressor,\n",
    "                     control_regressor))\n",
    "\n",
    "# Plot the design matrix:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ade5941-d8b0-43f0-9543-8b9e0a5a43b2",
   "metadata": {},
   "source": [
    "Confirm (using `assert`) that the number of time points (i.e., TRs) in both $X$ and $Y$ match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f533b8c5-b7fd-49e1-a1a0-594ece1f9bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that number of TRs in X and Y match:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63737241-9231-446f-9b7c-5f88ef301097",
   "metadata": {},
   "source": [
    "#### Running the regression model\n",
    "We're finally ready to run our regression. We'll use multiple linear regression (often referred to as a general linear model or \"GLM\" in the fMRI literature; [Friston et al., 1994](https://doi.org/10.1002/hbm.460020402)) to quantify how much a given voxel responds to (i.e., is predicted by) the regressor for a given experimental condition. Here our model only has two regressors corresponding to the language task (meaningful sentences) and the control task (meaningless strings). We'll use `lstsq` from `np.linalg` to run ordinary least-squares (OLS) regression and extract the regression coefficient `b` (or \"beta\") from the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82961af2-31a6-4065-b252-994dfae4ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run regression with basic OLS\n",
    "b, _, _, _ = np.linalg.lstsq(X, Y, rcond=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e69c4-20b7-479b-980a-4848ea8915d4",
   "metadata": {},
   "source": [
    "This is a \"mass univariate analysis\" in the sense that we are effectively computing a separate regression model independently for each of $\\approx20\\text{,}000$ voxels (even when we solve the regression equation simultaneously across all voxels using matrix algebra; you could confirm this by running the regression separately on each voxel in a loop). Make sure you understand the shape of the `b` coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36834b29-1667-4c31-8862-525320189157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the shape of the beta coefficients:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d5cdd-d674-4dae-a371-23e91238242d",
   "metadata": {},
   "source": [
    "Typically we don't want to just look at the coefficient for the `language` task in isolation—we want to directly compare the `language` task (meaningful sentences) to the `control` task (meaningless strings). Compute a \"contrast\" (i.e., the difference) between the language and control coefficients (ignore the intercept coefficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e85e1-1571-459e-af96-4f150fb2148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute contrast between language and control condition:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c549665-2cd1-47d6-82e7-c2aff752569d",
   "metadata": {},
   "source": [
    "To convert this contrast values for the masked voxels back into a full 3-dimensional brain image, we can use the `.inverse_transform` method of our `masker` object. Plot the resulting contrast map `plot_glass_brain` from `nilearn.plotting` with `display_mode='lzr'`, `cmap='RdBu_r'`, `symmetric_cbar=True`, and `plot_abs=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0926ed-b57d-45f2-b927-4fdd6b507f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use masker to convert contrasts back into 3D brain image:\n",
    "\n",
    "# Plot contrast map:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768e7c3-c58f-48ad-a67b-54ebe260abea",
   "metadata": {},
   "source": [
    "Use the Markdown cell below to write out a brief reflection (a couple sentences) interpreting what you can learn from this brain image. Consider the following points in your response:\n",
    "\n",
    "* What exactly can we conclude from this contrast between meaningful sentences and sequences of meaningless consonant strings? What other tasks or contrasts might be useful in isolating language areas?\n",
    "\n",
    "* Try re-running the above cells for, e.g., `sub-02` and look the resulting contrast map. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78a7f13-fc58-4fcb-916e-169a1f94a3ca",
   "metadata": {},
   "source": [
    "#### Reflection\n",
    "*Your response here!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2392e59-0958-441a-a53c-885380e60bed",
   "metadata": {},
   "source": [
    "*Bonus:* If you want to get even more into the nuts-and-bolts of linear algebra, try re-running the ordinary least-squares (OLS) regression using only [matrix algebra](https://en.wikipedia.org/wiki/Ordinary_least_squares#Matrix/vector_formulation) instead of `np.linalg.lstsq`. Define a function the `X` and `Y` as input and returns `b` according to the following equation: $\\hat{\\beta} = (X^{\\top} X)^{-1} X^{\\top} Y$. (You can use `@` for matrix multiplication and `np.linalg.inv` to compute the inverse of a matrix.) Check that your new matrix of coefficients is effectively equal to the previous `b` (e.g., using `np.allclose`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f63f93c-5eb0-46f3-b0dc-41e8015d6995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: compute weights using matrix algebra:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc7b02-c3c6-4982-bbe8-a5d6e01d2d06",
   "metadata": {},
   "source": [
    "### Part 2: Language localizer \"at scale\"\n",
    "In this section, we'll move a bit closer to a production-level analysis across all $N = 10$ subjects. We'll take advantage of some higher-level *Nilearn* functions and leverage the fact that this dataset is organized according to the Brain Imaging Data Structure (BIDS; [Gorgolewsky et al., 2016](https://doi.org/10.1038/sdata.2016.44)). First, we'll use `first_level_from_bids` from `nilearn.glm.first_level` to automagically initialize subject-level regression models alongside the BOLD files, events tables, and confounds time series. Have a look at each of these lists to make sure you understand what we're working with. Note that we'll also apply some spatial smoothing in this analysis (`smoothing_fwhm=6`) to boost functional correspondence across individual subjects (at the expense of individual functional–anatomical specificity). *Nilearn* will also include the head motion time series from `confounds` as nuisance regressors in the GLM to minimize the effects of head motion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89fcb93-2549-4ef9-977e-bfec05124f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all components of dataset based on BIDS\n",
    "# (ignore warnings about StartTime and slice timing)\n",
    "from nilearn.glm.first_level import first_level_from_bids\n",
    "\n",
    "N = 10\n",
    "task = 'languagelocalizer'\n",
    "smoothing_fwhm = 8\n",
    "\n",
    "model_list, bold_list, events_list, confounds_list = first_level_from_bids(\n",
    "    dataset.data_dir,\n",
    "    task,\n",
    "    img_filters=[('desc', 'preproc')],\n",
    "    space_label='',\n",
    "    mask_img=mni_mask_r,\n",
    "    smoothing_fwhm=smoothing_fwhm,\n",
    "    n_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd594f1-45a1-452c-b70a-2dfed6067ea0",
   "metadata": {},
   "source": [
    "Now that we've compiled all the necessary ingredients, we'll loop through each subject, fit their GLM, and compute the contrasts. This is basically the same thing we did in Part 1, but all at once (with a few additional bells and whistles). This cell may take a couple minutes to run: you're fitting $10$ (subjects) $\\times$ $20\\text{,}000$ (voxels) regression models! (in addition to all the masking, smoothing, detrending, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe03fd7-712c-4710-9a8f-71ae02c60044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip all components together for loop\n",
    "inputs = zip(model_list, bold_list, events_list, confounds_list)\n",
    "\n",
    "# Loop through each subject and run regression model\n",
    "models, contrasts = [], []\n",
    "for n, (model, bold_f, events, confounds) in enumerate(inputs):\n",
    "    model.fit(bold_f, events, confounds)\n",
    "    contrast = model.compute_contrast(\"language-string\")\n",
    "    models.append(model)\n",
    "    contrasts.append(contrast)\n",
    "    print(f\"Finished first-level model for subject {n + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208b844-4ffa-4f84-bddf-4fbbbf4ae66e",
   "metadata": {},
   "source": [
    "Have a look at one of the fitted models. In the *Parameters* drop-down, you can see all of the defaults in *Nilearn*'s regression model. For example: we're using the `glover` HRF we used above; we're using cosine bases to account for slow drifts; we're using a high-pass filter with a cutoff at $0.01~\\mathrm{Hz}$; we're using an AR(1) autoregressive model for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a12e4-4551-4803-9f7f-ca7e69287ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect one of the fitted models:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0507bae0-9d6a-41d6-b1c7-4f804b99999e",
   "metadata": {},
   "source": [
    "Let's visualize the data for all subjects at once. Using `plt.subplots` with a through the figure axes and contrast maps, you should be able to create a $2 \\times 10$ grid of `plot_glass_brain` plots for the $N=10$ subjects. Specify `axes=ax` and `display_mode='l'` in your `plot_glass_brain` call, and remember to set `plot_abs=False` and `color_bar=False`. You can use the `n` variable to set the `title` of each subplot to the subject number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2d515-9faf-4709-a40a-cf85ba285a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a montage of all subject-level contrast maps\n",
    "fig, axs = plt.subplots(2, 5, figsize=(12, 4))\n",
    "for n, (ax, contrast) in enumerate(zip(axs.ravel(), contrasts)):\n",
    "\n",
    "    # Set up plot_glass_brain inside loop here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2120201-e4ad-4ad1-a3b0-3f2a7edf237e",
   "metadata": {},
   "source": [
    "How can we evaluate the strength of this contrast across subjects? The most straightforward approach to statistically assess a group-level effect would be to perform a one-sample *t*-test across the subject-level contrast maps. Note that this is mathematically equivalent to performing a paired *t*-test for the difference between the language and control coefficients. First, use `vstack` and `masker.transform` to create a two-dimensional stack of contrast maps where axis 0 corresponds to the $N = 10$ subjects and axis 1 corresponds to the voxels in the mask. Next, using `np.mean`, compute the mean contrast across subjects and hold onto it for later. Finally, you can use `ttest_1samp` from `scipy.stats` to perform a two-sided t-test against the population mean of zero contrast. This should yield a set of *t*- and *p*-values for each voxel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22e019-145b-473e-9cfd-5e7987405bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack masked contrast maps across subjects:\n",
    "\n",
    "# Compute the mean across subjects:\n",
    "\n",
    "# Use ttest_1samp to compute t- and p-values:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f24c9f-7faf-4fe2-a9a7-32ff546d33b4",
   "metadata": {},
   "source": [
    "To decide which voxels show a statistically significant effect, we can't just threshold this map at $p < .05$. For $\\sim20\\text{,}000$ voxels, we would expect to see $\\sim1\\text{,}000$ significant just by chance (yikes). There are variety of ways to control for multiple tests (e.g., [Eklund et al., 2016](https://doi.org/10.1073/pnas.1602413113)), but here we'll simply control the false discovery rate (FDR; [Benjamini & Hochberg, 1995](https://doi.org/10.1111/j.2517-6161.1995.tb02031.x)) at $.05$. Use `multipletests` from `statsmodels.stats.multitest` with `method='fdr_bh'` to identify which voxels remain significant when controlling FDR at $.05$. Call this mask of FDR-significant voxels `reject` (i.e., \"rejecting\" the null hypothesis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02de430-cdca-4557-8705-42a9ae9f45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify significant voxels when controlling FDR:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df73ed-2443-4762-8485-fb5eea3d5ed4",
   "metadata": {},
   "source": [
    "Given a mask of significant voxels (called `reject`), we can construct a final result map. We'll start with a map of zeros and insert the mean contrast value for the voxels where the effect is significant at $p_\\mathrm{FDR} < .05$. Then, we'll use `masker.inverse_transform` to convert these voxels back into a 3-dimensional image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c812a5f-3db3-4c80-8555-2532b838ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create result map thresholded by FDR\n",
    "result = np.zeros(t.shape)\n",
    "result[reject] = contrast_mean[reject]\n",
    "\n",
    "# Transform masked voxels back into 3D image\n",
    "result_img = masker.inverse_transform(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f80ae77-4127-475e-9d6c-e26543feadeb",
   "metadata": {},
   "source": [
    "Plot the `result_img` using `plot_glass_brain` with `display_mode='lzr'`, `cmap='RdBu_r'`, `symmetric_cbar=True`, `plot_abs=False`, and `threshold=0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc0cca-1d6a-4d2f-9899-8cc0825b5fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the FDR-thresholded brain image:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8386c71d-b24e-4abf-8860-c0b2fb6c39b0",
   "metadata": {},
   "source": [
    "We can achieve the same effect a bit more succintly using *Nilearn*. First, we set up a group-level analysis using `SecondLevelModel` based on the same contrast we used in the subject-level analysis. Using `.compute_contrast`, we can ask for *z*-scores, *t*-values, *p*-values, and the effect sizes (coefficients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a19be55-a48b-49e6-b962-6de70caf5f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "\n",
    "# Compile first-level models as input to group analysis\n",
    "group_input = models\n",
    "\n",
    "# Initialize and fit group-level model\n",
    "group_model = SecondLevelModel(n_jobs=2)\n",
    "group_model = group_model.fit(group_input)\n",
    "\n",
    "# Compute contrasts and extract statistics\n",
    "results = group_model.compute_contrast(\n",
    "    first_level_contrast='language-string',\n",
    "    second_level_stat_type='t', output_type='all')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00dc2e6-d74e-427b-94aa-6fceb840fc2f",
   "metadata": {},
   "source": [
    "Next, we can use `threshold_stats_img` to obtain an FDR-corrected statistical threshold. We binarize this threshold and multiply it by the effect size map to replicate the same result from above. (Note that sometimes published papers will report thresholded effect size maps, but sometimes they will report thresholded *t*- or *z*-value maps.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93298a-099d-4480-a235-6744fd00b0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm import threshold_stats_img\n",
    "from nilearn.image import binarize_img, math_img\n",
    "\n",
    "# Use FDR correction to identify statistically significant voxels\n",
    "threshold_map, threshold = threshold_stats_img(\n",
    "    results['z_score'], alpha=0.05, height_control='fdr',\n",
    "    mask_img=mni_mask_r, two_sided=True)\n",
    "\n",
    "# Use multiplication hack to threshold effect size image\n",
    "result_img = math_img('img1 * img2',\n",
    "                      img1=results['effect_size'],\n",
    "                      img2=binarize_img(threshold_map, two_sided=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25859aec-8886-4c04-b4b9-3187265dbd9e",
   "metadata": {},
   "source": [
    "Finally, let's plot the resulting statistical map and compare it to the plot we created above by \"manually\" performing the t-test and FDR correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45783ba2-78cc-4f55-be17-6a5af160c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the FDR-thresholded brain image\n",
    "plot_glass_brain(result_img, plot_abs=False,  symmetric_cbar=True,\n",
    "                 display_mode='lzr', cmap='RdBu_r', colorbar=False,\n",
    "                 threshold=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031ef7d4-af5e-49ce-a8e6-10c4612c43e0",
   "metadata": {},
   "source": [
    "Write a brief reflection (a couple sentences) about what you learned from these analyses. Consider the following points in your response:\n",
    "\n",
    "* Do these results match with what you would expect from the literature you've read so far?\n",
    "\n",
    "* Observe the variability in effect sizes and cortical topography across individual subjects. How can we better accommodate these individual differences in functional topography?\n",
    "\n",
    "* How do you interpret the \"significantly\" negative activations (blue) in posterior medial areas (e.g., precuneus) and anterior prefrontal areas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d289901f-ccda-4def-b7c5-e12587b2756f",
   "metadata": {},
   "source": [
    "#### Reflection\n",
    "*Your response here!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c0901-c741-4013-a41c-cc91f120f8c5",
   "metadata": {},
   "source": [
    "### References\n",
    "* Abraham, A., Pedregosa, F., Eickenberg, M., Gervais, P., Mueller, A., Kossaifi, J., Gramfort, A., Thirion, B., & Varoquaux, G. (2014). Machine learning for neuroimaging with scikit-learn. *Frontiers in Neuroinformatics*, *8*, 14. https://doi.org/10.3389/fninf.2014.00014\n",
    "\n",
    "* Benjamini, Y., & Hochberg, Y. (1995). Controlling the false discovery rate: a practical and powerful approach to multiple testing. *Journal of the Royal Statistical Society: Series B (Methodological)*, *57*(1), 289–300. https://doi.org/10.1111/j.2517-6161.1995.tb02031.x\n",
    "\n",
    "* Eklund, A., Nichols, T. E., & Knutsson, H. (2016). Cluster failure: why fMRI inferences for spatial extent have inflated false-positive rates. *Proceedings of the National Academy of Sciences*, *113*(28), 7900–7905. https://doi.org/10.1073/pnas.1602413113\n",
    "\n",
    "* Fedorenko, E., Hsieh, P. J., Nieto-Castañón, A., Whitfield-Gabrieli, S., & Kanwisher, N. (2010). New method for fMRI investigations of language: defining ROIs functionally in individual subjects. *Journal of Neurophysiology*, *104*(2), 1177–1194. https://doi.org/10.1152/jn.00032.2010\n",
    "\n",
    "* Fedorenko, E., Ivanova, A. A., & Regev, T. I. (2024). The language network as a natural kind within the broader landscape of the human brain. *Nature Reviews Neuroscience*, *25*(5), 289–312. https://doi.org/10.1038/s41583-024-00802-4\n",
    "\n",
    "* Friston, K. J., Holmes, A. P., Worsley, K. J., Poline, J. P., Frith, C. D., & Frackowiak, R. S. (1994). Statistical parametric maps in functional imaging: a general linear approach. *Human Brain Mapping*, *2*(4), 189–210. https://doi.org/10.1002/hbm.460020402\n",
    "\n",
    "* Glover, G. H. (1999). Deconvolution of impulse response in event-related BOLD fMRI. *NeuroImage*, *9*(4), 416–429. https://doi.org/10.1006/nimg.1998.0419\n",
    "\n",
    "* Gorgolewski, K. J., Auer, T., Calhoun, V. D., Craddock, R. C., Das, S., Duff, E. P., ... & Poldrack, R. A. (2016). The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments. *Scientific Data*, *3*, 160044. https://doi.org/10.1038/sdata.2016.44\n",
    "\n",
    "* Pallier, C. (2019). fMRI language localizer demo dataset. *OSF*. https://osf.io/k4jp8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
