{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e43194-2035-4170-b2b3-e6b85004f5d1",
   "metadata": {},
   "source": [
    "# Lab 3: Voxelwise encoding models\n",
    "In this notebook, we'll develop voxelwise encoding models (VEMs) for mapping embeddings from a language model onto human brain activity during natural language comprehension. We'll first construct VEMs for one set of language features, then use banded ridge regression to compare to different sets of langauge features.  Encoding models are estimated using banded ridge regression with cross-validation—this allows us to predict brain activity from word embeddings for left-out segments of data.\n",
    "\n",
    "*Acknowledgments:* This tutorial draws heavily on work by co-author Zaid Zada (e.g. code from [Zada et al., 2023](https://doi.org/10.1016/j.neuron.2024.06.025), [2025](https://doi.org/10.1016/j.neuron.2025.11.004)) as well as Gallant Lab's [voxelwise modeling tutorials](https://gallantlab.org/voxelwise_tutorials/index.html) ([Dupré La Tour et al., 2022](https://doi.org/10.1016/j.neuroimage.2022.119728), [2025](https://doi.org/10.1162/imag_a_00575)).\n",
    "\n",
    "---\n",
    "\n",
    "First, you'll need to install a few more Python packages for new software used in this tutorial. Run the following lines in the command line. You may want to shut down your Jupyter Lab entirely and restart to make sure the progress bar widget works properly.\n",
    "\n",
    "`conda install -c conda-forge gensim transformers accelerate ipywidgets`<br>\n",
    "`pip install voxelwise_tutorials`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18750695-0b3a-4000-83b1-73727023dca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Explicitly set Mac locale character type to English\n",
    "!export LC_CTYPE=\"en_US.UTF-8\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1495c9-8e36-43cc-a0d6-4cf6d47f32e4",
   "metadata": {},
   "source": [
    "## Natural language comprehension fMRI dataset\n",
    "In our first example, we'll use fMRI data collected for a single subject listening to a spoken story called \"[I Knew You Were Black](https://themoth.org/stories/i-knew-you-were-black)\" by Carol Daniel. This dataset should be quick (less than a minute) to download! These data are part of the publicly available [Narratives](https://github.com/snastase/narratives) collection ([Nastase et al., 2021](https://doi.org/10.1038/s41597-021-01033-3)). This dataset has been preprocessed using [fMRIPrep](https://fmriprep.org/en/stable/) with confound regression in [AFNI](https://afni.nimh.nih.gov/). The functional data have been spatially normalized to a template in MNI space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226b672-23a8-45e9-a009-500ef82d7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from Zenodo if we don't already have it\n",
    "if not exists('encling-data.tgz'):\n",
    "    # Use wget instead on a Linux machine\n",
    "    # or download by hand: https://zenodo.org/records/8216229\n",
    "    !curl -O https://zenodo.org/records/8216229/files/encling-data.tgz\n",
    "    !tar -xvzf encling-data.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab1def0-7c55-4996-8026-fb72b43815af",
   "metadata": {},
   "source": [
    "To reduce computational demands, we compute parcel-wise VEMs using a cortical parcellation containing 400 parcels ([Schaefer et al., 2018](https://doi.org/10.1093/cercor/bhx179)). `NiftiLabelsMasker` provides an easy way to extract the mean BOLD time series from each parcel in the atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda4f46-6a5f-440d-a5b0-9960da5b72fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.datasets import fetch_atlas_schaefer_2018\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "\n",
    "# Preprocessed fMRI data from Narratives\n",
    "func_fn = ('encling-data/sub-284_task-black_space-MNI152NLin2009cAsym_res-native_desc-clean_bold.nii.gz')\n",
    "\n",
    "# Fetch Schaefer atlas with 400 parcels and 17 Yeo networks\n",
    "n_parcels = 400\n",
    "atlas = fetch_atlas_schaefer_2018(n_rois=n_parcels, yeo_networks=17, resolution_mm=2)\n",
    "\n",
    "# Initialize labels masker with atlas parcels\n",
    "masker = NiftiLabelsMasker(atlas.maps, labels=atlas.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3535805-7930-4b52-a83c-68d9124b8a24",
   "metadata": {},
   "source": [
    "Use the masker's `.fit_transform()` method to extract parcel-level time series from `func_fn`. In my code, I called the output object `func_parcels`. Inspect the shape of your `func_parcels` and make sure you know what each dimension corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d04c50-2fe3-4f81-a4c6-ee0448d0a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit masker to extract mean time series for parcels:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f27a2c-2f96-4635-bedf-83c5d98f7c2f",
   "metadata": {},
   "source": [
    "To orient ourselves, let's plot the time series from an example parcel in anterior superior temporal cortex. Use `example_parcel = 195` to index the time series from `func_parcels` and plot using matplotlib's `plot()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7716c-02aa-4044-9252-fd40648dddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series for an example parcel 195:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3673d4fd-4626-4d14-a3d3-540edf9bc430",
   "metadata": {},
   "source": [
    "To transform parcel-level data back into a brain image, we'll use the `masker`'s `.inverse_transform()` method; this will allow us to visualize the location of the example parcel on the brain. Create an empty array of zeros the same shape as `func_parcels` and change the value at the `example_parcel` index to `1`. Then, use `.inverse_transform()` and plot the resulting brain image using `plot_stat_map` from `nilearn.plotting`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad85cb-dea6-457f-acec-13a2539c5a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct simple atlas map with 1 at parcel 195:\n",
    "\n",
    "# Invert masker transform to project onto brain:\n",
    "\n",
    "# Plot parcel on MNI atlas:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad734fb9-6bf7-435c-8aa4-860098707880",
   "metadata": {},
   "source": [
    "## Extracting word embeddings\n",
    "In the following sections, we extract two types of vectors—called \"word embeddings\"—capturing the meaning of the words in our transcript. In both cases, words are encoded as vectors of continuous numeric values in a high-dimensional embedding space where each dimension corresponds to an internal feature of the model. Words that are similar to each other are located nearing to each other in this embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9064ce37-5a0f-470a-a9cc-ba6462b6c2f1",
   "metadata": {},
   "source": [
    "### Static word embeddings\n",
    "In the first case, we'll retrieve static, noncontextual (lexical) word embeddings from a pre-trained model called word2vec ([Mikolov et al., 2013](https://doi.org/10.48550/arXiv.1301.3781)). This word2vec model was (pre)trained the Google News corpus containing approximately 100 billion words. We use a 300-dimensional word2vec model; that is, the hidden layer contains 300 units, resulting in 300-long word vectors. The word2vec embeddings are considered static embeddings because they capture the global meaning of a given word (based on its co-occurrence with other words), and each occurrence of a given word receives the same embedding regardless of the surrounding context. The model is about 2 GB in size, so it may take some time to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d155f4ef-b39d-4f5a-9a1b-aed762903425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "# Download 300-dimensional word2vec embeddings\n",
    "model_name = 'word2vec-google-news-300'\n",
    "n_features = 300\n",
    "\n",
    "model = gensim.downloader.load(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2921a3-cb32-4073-b0cd-95fe0fd2422c",
   "metadata": {},
   "source": [
    "This model is structured like a dictionary with strings (words) as keys and vectors (embeddings) as values. Try looking up an example word and inspect the shape of the corresponding embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e38de-43e2-4e95-89fc-733e87f98ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape of embedding for example word:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49967caa-652e-40bd-b1e6-db8387c81f41",
   "metadata": {},
   "source": [
    "Next, for every word in our transcript, we'll retrieve the corresponding embedding. We'll ignore any words that are not found in the word2vec vocabulary. We'll add these embeddings into our transcript and resave the transcript for easier loading later on. Make sure you understand what each line of code is doing in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bd2f06-68a2-4e5e-90cc-2362ff9af04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in transcript CSV file\n",
    "transcript_f = 'encling-data/black_transcript.csv'\n",
    "transcript_w2v = pd.read_csv(transcript_f)\n",
    "\n",
    "# Convert words to lowercase\n",
    "transcript_w2v['word'] = transcript_w2v.word.str.lower()\n",
    "\n",
    "# Function to extract embeddings if available\n",
    "def get_vector(word):\n",
    "    if word in model.key_to_index:\n",
    "        return model.get_vector(word, norm=True).astype(np.float32)\n",
    "    return np.nan\n",
    "\n",
    "# Extract embedding for each word\n",
    "transcript_w2v['embedding'] = transcript_w2v.word.apply(get_vector)  \n",
    "transcript_w2v = transcript_w2v.astype({'onset': 'float32', 'offset': 'float32'}, copy=False)\n",
    "\n",
    "# Print out words not found in vocabulary\n",
    "print(f'{(transcript_w2v.embedding.isna()).sum()} words not found:')\n",
    "print(transcript_w2v.word[transcript_w2v.embedding.isna()].value_counts())\n",
    "\n",
    "# Save transcript with embeddings using pickle\n",
    "with open('black_w2v.pkl', 'wb') as f:\n",
    "    pickle.dump(transcript_w2v, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c77dae6-5bd1-4667-b287-0fb1752df166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload transcript with embeddings if already generated\n",
    "transcript_f = 'black_w2v.pkl'\n",
    "if exists(transcript_f):\n",
    "    with open(transcript_f, 'rb') as f:\n",
    "        transcript_w2v = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e378c-8fce-4876-b97d-31e5681785c4",
   "metadata": {},
   "source": [
    "Have a look at the resulting `transcript_w2v` table created by the cells above. Check that the structure makes sense and confirm the total number of words in the transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b62cd36-59ee-4cae-8b9d-bcd1914f2d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the transcript:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec25e9-9320-4c1e-9919-69a96b06ab8c",
   "metadata": {},
   "source": [
    "### Contextual word embeddings\n",
    "In the second case, we'll extract contextual word embeddings from an autoregressive (or \"causal\") large language model (LLM) called GPT-2 ([Radford et al., 2019](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)). GPT-2 relies on the Transformer architecture to sculpt the embedding of a given word based on the preceding context. The model is composed of a repeated circuit motif—called \"self-attention heads\"—by which the model can \"attend\" to representations of previous words in the context window when determining the meaning of the current word. This GPT-2 implementation is composed of 12 layers, each of which contains 12 attention heads that influence the embedding as it proceeds to the subsequent layer. The embeddings at each layer of the model comprise 768 features and the context window includes the preceding 1024 tokens. Note that certain words will be broken up into multiple tokens; we'll need to use GPT-2's \"tokenizer\" to convert words into the appropriate tokens. GPT-2 has been (pre)trained on large corpora of text according to a simple self-supervised objective function: predict the next word (token) based on the prior context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a1872-ab77-4d30-a374-062e761c2f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "model_name = 'gpt2'\n",
    "n_features = 768\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Print out model architecture\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26c6c1-d92b-4102-b124-fc1e54172523",
   "metadata": {},
   "source": [
    "In the Markdown cell below, answer the following questions:\n",
    "1. What does the `50257` correspond to?\n",
    "2. What does the `1024` correspond to?\n",
    "3. What does the `768` correspond to?\n",
    "4. What does the `12` in `12 x GPT2Block` correspond to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7248cb8f-d81e-41ff-b4ed-158563cf5a1a",
   "metadata": {},
   "source": [
    "*Your response here!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c2d245-f7fb-43df-841a-13076515a5e3",
   "metadata": {},
   "source": [
    "Running the model on our transcript will be faster if GPUs are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe9b43-bbde-49ba-a971-3482af83cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get device for running model (e.g. MacOS 'mps')\n",
    "device = (\n",
    "    'cuda'\n",
    "    if torch.cuda.is_available()\n",
    "    else 'mps'\n",
    "    if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "\n",
    "# You may need to force CPU on Mac, but try with 'mps' first\n",
    "# device = 'cpu'\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c385b0c6-050c-4230-9434-572bc1cb8dc2",
   "metadata": {},
   "source": [
    "Now, we reload our original transcript. We'll run the tokenizer to convert the words into subword tokens for input to GPT-2. We need to keep track of the word indices when we run the tokenizer; we'll get an embedding for each token, but we'll want to recombine (i.e. average) embeddings for words split into multiple tokens. These tokens are supplied to GPT-2 as integer IDs corresponding to words the GPT-2 vocabulary (which contains approximately 50,000 words). Make sure you know what each line of code is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b1802-92c6-4e91-b474-e2b175c88515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload in transcript CSV file\n",
    "transcript_f = 'encling-data/black_transcript.csv'\n",
    "transcript_gpt2 = pd.read_csv(transcript_f)\n",
    "\n",
    "# Insert explicit index column for reference\n",
    "transcript_gpt2.insert(0, 'word_index', transcript_gpt2.index.values)\n",
    "\n",
    "# Tokenize words into lists of tokens\n",
    "transcript_gpt2['token'] = transcript_gpt2.word.apply(tokenizer.tokenize)\n",
    "\n",
    "# \"Explode\" lists of token subwords into long format\n",
    "transcript_gpt2 = transcript_gpt2.explode('token', ignore_index=True)\n",
    "\n",
    "# Convert tokens to token IDs for input to model\n",
    "transcript_gpt2['token_id'] = transcript_gpt2.token.apply(tokenizer.convert_tokens_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54bbd95-c89e-4f17-8d31-e27952e9a347",
   "metadata": {},
   "source": [
    "Since our transcript contains more tokens than can be contained in GPT-2's context window, we'll aggregate the tokens into multiple `samples` matching the width of the context window (1024 tokens) and spanning the full length of the transcript. On some systems, we may be able to use the `accelerate` package to speed up model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d76422-a21b-4679-9a6f-b3cf777c0d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all token IDs into list\n",
    "token_ids = transcript_gpt2.token_id.tolist()\n",
    "\n",
    "# Extract context window width for model\n",
    "max_len = tokenizer.model_max_length\n",
    "\n",
    "# Compile into lists of tokens within each context window\n",
    "samples = []\n",
    "token_ids = torch.tensor(transcript_gpt2.token_id.tolist(), dtype=torch.long)\n",
    "samples.append(token_ids[0:max_len])\n",
    "for i in range(max_len+1, len(token_ids)+1):\n",
    "    samples.append(token_ids[i-max_len:i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169bb016-0cb2-4269-b5ab-ff22f4323c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "# Initialize accelerator and free memory\n",
    "accelerator = Accelerator()\n",
    "accelerator.free_memory()\n",
    "\n",
    "# Send model to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a4f4a0-066c-45ff-8d7f-653860ad4ee3",
   "metadata": {},
   "source": [
    "Finally, we'll use a PyTorch `DataLoader` to supply token IDs to the model in batches and extract the embeddings. In addition to the embeddings, we'll also extract several other features of potential interest from the model. As GPT-2 proceeds through the text, it generates a probability distribution (the `logits` extracted below) across all words in the vocabulary with the goal of correctly predicting the next word. We can use this probability distribution to derive other features of the model's internal computations. We'll extract the following features from GPT-2:\n",
    " \n",
    "* `embeddings`: the 768-dimensional contextual embedding capturing the meaning of the current word\n",
    "* `top_guesses`: the highest probability word GPT-2 predicts for the current word\n",
    "* `ranks`: the rank of the correct word given probabilities across the vocabulary\n",
    "* `true_probs`: the probability at which GPT-2 predicted the current word\n",
    "* `entropies`: how the uncertain GPT-2 was about the current word\n",
    "  * low entropy indicates that the probability distribution was \"focused\" on certain words\n",
    "  * high entropy indicates the  probability distribution was more uniform/dispersed across words\n",
    "\n",
    "*Note:* This upcoming cell may take a few minutes to run on a GPU (and longer on a CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3fc1c7-a1fc-4506-b335-4e1c2699e46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a batch size for the data loader\n",
    "batch_size = 4\n",
    "\n",
    "# Extract a late-intermediate layer from GPT-2\n",
    "layer = 8\n",
    "\n",
    "# Extract embeddings and other model features\n",
    "embeddings = []\n",
    "top_guesses = []\n",
    "ranks = []\n",
    "true_probs = []\n",
    "entropies = []\n",
    "with torch.no_grad():\n",
    "    data_loader = torch.utils.data.DataLoader(samples, batch_size=batch_size,\n",
    "                                              shuffle=False)\n",
    "\n",
    "    # Loop through samples and extract embeddings\n",
    "    for i, batch in enumerate(tqdm(data_loader)):\n",
    "        output = model(batch.to(device), output_hidden_states=True)\n",
    "        logits = output.logits  # torch.Size([2, 1024, 50257])\n",
    "        states = output.hidden_states[layer]\n",
    "\n",
    "        # Extract all embeddings/features for first context window\n",
    "        if i == 0:\n",
    "            true_ids = batch[0, :]\n",
    "            brange = list(range(len(true_ids)-1))\n",
    "            logits_order = logits[0].argsort(descending=True, dim=-1)\n",
    "            batch_top_guesses = logits_order[:-1, 0]\n",
    "            batch_ranks = torch.eq(logits_order[:-1],\n",
    "                                   true_ids.reshape(-1,1)[1:].to(device)).nonzero()[:, 1]\n",
    "            batch_probs = logits[0, :-1].softmax(-1)\n",
    "            batch_true_probs = batch_probs[brange, true_ids[1:]]\n",
    "            batch_entropy = torch.distributions.Categorical(probs=batch_probs).entropy()\n",
    "            batch_embeddings = states[0]\n",
    "\n",
    "            top_guesses.append(batch_top_guesses.numpy(force=True))\n",
    "            ranks.append(batch_ranks.numpy(force=True))\n",
    "            true_probs.append(batch_true_probs.numpy(force=True))\n",
    "            entropies.append(batch_entropy.numpy(force=True))\n",
    "            embeddings.append(batch_embeddings.numpy(force=True))\n",
    "            \n",
    "            # Reset if there are samples remaining in this batch\n",
    "            if batch.size(0) == 1:\n",
    "                continue\n",
    "            logits = logits[1:]\n",
    "            states = states[1:]\n",
    "            batch = batch[1:]\n",
    "\n",
    "        # Extract embeddings/features for last word in subsequent windows\n",
    "        true_ids = batch[:, -1]\n",
    "        brange = list(range(len(true_ids)))\n",
    "        logits_order = logits[:, -2, :].argsort(descending=True)  # batch x vocab_size\n",
    "        batch_top_guesses = logits_order[:, 0]\n",
    "        batch_ranks = torch.eq(logits_order, true_ids.reshape(-1,1).to(device)).nonzero()[:, 1]\n",
    "        batch_probs = torch.softmax(logits[:, -2, :], dim=-1)\n",
    "        batch_true_probs = batch_probs[brange, true_ids]\n",
    "        batch_entropy = torch.distributions.Categorical(probs=batch_probs).entropy()\n",
    "        batch_embeddings = states[:, -1, :]\n",
    "\n",
    "        top_guesses.append(batch_top_guesses.numpy(force=True))\n",
    "        ranks.append(batch_ranks.numpy(force=True))\n",
    "        true_probs.append(batch_true_probs.numpy(force=True))\n",
    "        entropies.append(batch_entropy.numpy(force=True))\n",
    "        embeddings.append(batch_embeddings.numpy(force=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcf0ba-db5d-4946-9824-98ffbf0c7158",
   "metadata": {},
   "source": [
    "We'll recompile the features we extracted from GPT-2 into our transcript and save it for easier loading later on. We'll also summarize how accurately GPT-2 was able to predict upcoming words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ea22f-3554-4c65-a808-c53ac32a3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile outputs into transcript (logit derivatives must be shifted by 1)\n",
    "transcript_gpt2.loc[1:, 'rank'] = np.concatenate(ranks)\n",
    "transcript_gpt2.loc[1:, 'true_prob'] = np.concatenate(true_probs)\n",
    "transcript_gpt2.loc[1:, 'top_pred'] = np.concatenate(top_guesses)\n",
    "transcript_gpt2.loc[0, 'top_pred'] = tokenizer.bos_token_id\n",
    "transcript_gpt2.loc[1:, 'entropy'] = np.concatenate(entropies)\n",
    "transcript_gpt2['embedding'] = [e for e in np.vstack(embeddings)]\n",
    "\n",
    "# Reduce size of transcript\n",
    "transcript_gpt2 = transcript_gpt2.astype({'word_index': 'int32', 'onset': 'float32',\n",
    "                                      'offset': 'float32', 'token_id': 'int32',\n",
    "                                      'rank': 'float32', 'true_prob': 'float32',\n",
    "                                      'top_pred': 'int32', 'entropy': 'float32'}, copy=False)\n",
    "\n",
    "# Convert model's top predictions from token IDs to tokens\n",
    "transcript_gpt2['top_pred'] = transcript_gpt2.top_pred.apply(tokenizer.convert_ids_to_tokens)\n",
    "\n",
    "# Print out top-1 and top-10 word prediction accuracy\n",
    "print(f\"Top-1 accuracy: {(transcript_gpt2['rank'] == 0).mean():.3f}\")\n",
    "print(f\"Top-10 accuracy: {(transcript_gpt2['rank'] < 10).mean():.3f}\")\n",
    "\n",
    "# Save transcript with embeddings using pickle\n",
    "with open('black_gpt2.pkl', 'wb') as f:\n",
    "    pickle.dump(transcript_gpt2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234f3406-0fce-43fc-a1b2-84d1c5700401",
   "metadata": {},
   "source": [
    "Take a moment to think about and understand what the \"top-1 accuracy\" and \"top-10 accuracy\" numbers mean here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74dfe65-8db4-4e80-b99a-c1feb147225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload transcript with embeddings if already generated\n",
    "transcript_f = 'black_gpt2.pkl'\n",
    "if exists(transcript_f):\n",
    "    with open(transcript_f, 'rb') as f:\n",
    "        transcript_gpt2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488f005e-b495-48bd-b7eb-04b43a2c6b6f",
   "metadata": {},
   "source": [
    "Have a look at the resulting transcript with embeddings and make sure it matches your intuitions. Specifically, check the first 10 tokens using `transcript_gpt2.iloc[0:10]`. What word had the lowest entropy? (Put differently, what word was the model the least uncertain about before it occurred?) What did the model most strongly expect for this token and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bfc303-a933-4225-968e-c8cf58adf338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the transcript:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf47cdfb-7e6a-42a7-8ff7-fb86bd03b0b7",
   "metadata": {},
   "source": [
    "*Your response here!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df56bf7e-99e7-4bfa-b9d4-19645de276cc",
   "metadata": {},
   "source": [
    "## Estimating and evaluating encoding models\n",
    "In the following section, we'll use the embeddings extracted above to construct our encoding model. The goal of the encoding model is to evaluate how well we can predict brain activity from our representation of the stimulus in a given feature space ([Naselaris et al., 2011](https://doi.org/10.1016/j.neuroimage.2010.07.073)); in this case, our word embeddings encode linguistic content of the spoken story. We'll use linear regression to estimate a mapping from the word embeddings to the fMRI time series at each cortical parcel. Our model is very wide—containing hundreds or thousands of features or predictors (i.e. columns)—relative to the number of samples (i.e. TRs), so we are prone to overfitting. We'll rely on two fundamental principles of machine learning to mitigate the risks of overfitting. First, we'll introduce a *regularization* penalty (i.e. a particular kind of bias) into our regression equation to constrain how well the model (over)fits to the training data; ridge regression effectively \"squeezes\" the regression weights to reduce overfitting. Second, we will use *out-of-sample prediction* to evaluate the performance of our model; that is, we'll use cross-validation to train the model on subsets of the data and test the model on non-overlapping subsets of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a12e5d-e035-4115-8d5d-5b180e92850d",
   "metadata": {},
   "source": [
    "### Construct predictor matrix\n",
    "First, we need to resample our embeddings to ensure we have only vector per TR; the number of samples in our targets `Y` (parcelwise BOLD time series) and our predictors `X` must match. In this study, the fMRI data were acquired with 1.5-second TRs. For TRs with multiple embeddings (e.g. multiple words or tokens), we'll average the embeddings; for TRs with no embeddings (e.g. moments of silence), we'll insert zero vectors matching the length of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c126348-9615-4da7-b331-33408e662fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average embeddings per TR\n",
    "def construct_predictors(transcript_df, n_features, stim_dur, tr=1.5):\n",
    "\n",
    "    # Find total number of TRs\n",
    "    stim_trs = np.ceil(stim_dur / tr)\n",
    "\n",
    "    # Add column to transcript with TR indices\n",
    "    transcript_df['TR'] = transcript_df.onset.divide(tr).apply(np.floor).apply(int)\n",
    "    \n",
    "    # Compile the words within each TR\n",
    "    words_per_tr = transcript_df.groupby('TR')['word'].apply(list)\n",
    "    \n",
    "    # Average the embeddings within each TR\n",
    "    embeddings_per_tr = transcript_df.groupby('TR')['embedding'].mean()\n",
    "    \n",
    "    # Loop through TRs\n",
    "    words_trs = []\n",
    "    embeddings_trs = []\n",
    "    for t in np.arange(stim_trs):\n",
    "        if t in words_per_tr:\n",
    "            words_trs.append(words_per_tr[t])\n",
    "    \n",
    "            # Fill in empty TRs with zero vectors\n",
    "            if embeddings_per_tr[t] is not np.nan:\n",
    "                embeddings_trs.append(embeddings_per_tr[t])\n",
    "            else:\n",
    "                embeddings_trs.append(np.zeros(n_features))\n",
    "        else:\n",
    "            words_trs.append([])\n",
    "            embeddings_trs.append(np.zeros(n_features))\n",
    "    \n",
    "    embeddings = np.vstack(embeddings_trs)\n",
    "    return embeddings\n",
    "\n",
    "# word2vec embeddings are 300-dimensional\n",
    "X_w2v = construct_predictors(transcript_w2v, 300, 800, tr=1.5)\n",
    "\n",
    "# GPT-2 embeddings are 768-dimensional\n",
    "X_gpt2 = construct_predictors(transcript_gpt2, 768, 800, tr=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b7af2-23e7-4d21-926b-aa299356c5c4",
   "metadata": {},
   "source": [
    "Visualize both matrices of model features using `matshow`. Make sure you understand what the rows and columns correspond to (i.e., label your axes properly). You may want to use `zscore` from `scipy.stats` to mean-center and rescale the variance of each feature (column) prior to plotting the matrices (`vmin=-3` and `vmax=3` should work well with z-scored features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ac8ff-0f21-417f-8f0e-b5bb69eba999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize word embedding predictor matrices:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c4c3d8-5ff2-4e28-a7b2-94e19b9ce238",
   "metadata": {},
   "source": [
    "The fMRI data were acquired the 8 TRs of fixation both before and after the story stimulus. We'll trim these TRs off to ensure the fMRI data match the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3193e6c-60b2-4201-8b23-16bb8efb1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim fMRI data to match embeddings\n",
    "start_trs = 8\n",
    "end_trs = 8\n",
    "\n",
    "assert start_trs + X_w2v.shape[0] + end_trs == func_parcels.shape[0]\n",
    "\n",
    "Y_parcels = func_parcels[start_trs:-end_trs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc015975-429a-49ce-9787-610bddf5a62f",
   "metadata": {},
   "source": [
    "Check the shape of both sets of model features (`X_w2v` and `X_gpt2`) and the brain data (`Y_parcels`). The number of samples must match between the model features and our target variables for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2983694-72d9-41ef-9c34-a7989320c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of X and Y matrices:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b696635-a274-4ab5-8287-58cdd779f4f8",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "Next, we'll use ridge regression to predict the activity at each parcel from the word embeddings. For this section, we'll consider each feature space (i.e. word2vec, GPT-2) in isolation. We'll use a split-half outer cross-validation scheme where we train the model on half of the story and test the model on the other half. To search for the best-performing regularization parameter $\\lambda$ (`alpha` in scikit-learn convention), we'll perform 5-fold inner cross-validation within each training set using `KernelRidgeCV`; this will select the best parameter setting from the inner cross-validation fold within the training half to predict the test half. Higher `alpha` values increase regularization and reduce overfitting. Within each cross-validation fold, we'll apply two transforms: `StandardScaler` will be used to mean-center or z-score each column of the predictor matrix within the test set, then apply that transformation to the training set; `Delayer` will horiztonally stack lagged versions our predictor matrix to account for the hemodynamic lag. We use Himalaya's `KernelRidgeCV` (rather than scikit-learn's `RidgeCV`) because the multi-delayed version of the predictor matrix will be considerably wider than the number of fMRI samples. We'll combine these transforms and the estimator into a scikit-learn `Pipeline` that will run the whole analysis. This analysis qualitatively reproduces one of the core results from Huth and colleagues ([2016](https://doi.org/10.1038/nature17637)). Make sure you know what each line of code is doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb628d-02f5-44c4-858c-9e1cfc706783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from voxelwise_tutorials.delayer import Delayer\n",
    "from sklearn.model_selection import KFold\n",
    "from himalaya.kernel_ridge import KernelRidgeCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Split-half outer and inner cross-validation\n",
    "outer_cv = KFold(n_splits=2)\n",
    "inner_cv = KFold(n_splits=5)\n",
    "\n",
    "# Mean-center each feature (columns of predictor matrix)\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "# Create delays at 3, 4.5, 6, 7.5 seconds (1.5 s TR)\n",
    "delayer = Delayer(delays=[2, 3, 4, 5])\n",
    "\n",
    "# Ridge regression with alpha grid and nested CV\n",
    "alphas = np.logspace(1, 10, 10)\n",
    "ridge = KernelRidgeCV(alphas=alphas, cv=inner_cv)\n",
    "\n",
    "# Chain transfroms and estimator into pipeline\n",
    "pipeline = make_pipeline(scaler, delayer, ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df8eb6e-5b0a-450a-9277-4493732bf422",
   "metadata": {},
   "source": [
    "We'll run ridge regression separately for each feature space separately. Run the encoding analysis on the `word2vec` embeddings first and have a look at the results, then re-run the whole pipeline with the `GPT-2` embeddings. In the following cell, we loop through the outer cross-validation folds, fit the pipeline within each fold, then generate predictions. We use the model weights estimated from the training data to predict the brain activity from word embeddings for the test data. The `himalaya` implementation makes this surprisingly fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a958e71-0415-4ef0-af78-0590b392d2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select embeddings from one of the models\n",
    "X = X_gpt2\n",
    "\n",
    "# Loop through outer folds and estimate model\n",
    "Y_predicted = []\n",
    "for train, test in outer_cv.split(Y_parcels):\n",
    "    \n",
    "    # Fit pipeline with transforms and ridge estimator\n",
    "    pipeline.fit(X[train],\n",
    "                 Y_parcels[train])\n",
    "    \n",
    "    # Compute predicted response\n",
    "    predicted = pipeline.predict(X[test])\n",
    "    Y_predicted.append(predicted)\n",
    "    \n",
    "# Restack first and second half predictions\n",
    "Y_predicted = np.vstack(Y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786085b9-1c4f-4fd2-ab38-9380e49344c4",
   "metadata": {},
   "source": [
    "Check the shape of the model-based predictions and make sure you understand the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ba7add-5e18-4d59-a28b-16787554373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model-based predictions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d638c-2666-4149-a95a-8344a23b61ff",
   "metadata": {},
   "source": [
    "To evaluate the predictions of our model, we quantify the similarity between the predicted brain activity and the actual brain activity for the test data. Keeping with conventions in the literature, we use Pearson correlation to assess the match between predicted and actual brain activity. Use `correlation_score` from `himalaya.scoring` to compute the Pearson correlation between the model-predicted (`Y_predicted`) and actual test (`Y_parcels`) time series for each parcel. Note that in this simplified approach we stack the model-predicted test sets (both of which are generated based on a separate training set) and run one correlation across both the stacked test sets; alternatively, we could compute the correlation for each test set in the cross-validation loop above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b09578-4323-4419-a4b0-6447a7898cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions using correlation between predicted and actual time series:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35868fd-d3fd-40b7-904f-0fb2cd2a30e2",
   "metadata": {},
   "source": [
    "Print out the mean and maximum encoding performance values (out-of-sample correlations) across all parcels. Using `histplot` from seaborn (`sns`), create a simple histogram to visualize the correlation scores across all parcels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c119d1-61ca-4af1-a363-f84ce8ae8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print mean and maximum encoding performance across parcels:\n",
    "\n",
    "# Plot a histogram of prediction performance values:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08478d84-e216-43f6-a8b8-8be4fc4a9958",
   "metadata": {},
   "source": [
    "We can also introspect certain attributes of our fitted encoding model. For example, we may want to examine, the best alpha values selected from hyperparameter search, or we may want to recover the weight vectors estimated for a given training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d6ce1b-128b-4ff0-b71c-50d65bb2beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introspect fitted pipeline model for alphas and weights\n",
    "ridge_fitted = pipeline['kernelridgecv']\n",
    "best_alphas = ridge_fitted.best_alphas_\n",
    "weights = ridge_fitted.get_primal_coef(ridge_fitted.X_fit_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cee26a-a812-4c16-a4a1-e4a8f500c557",
   "metadata": {},
   "source": [
    "For illustrative purposes, let's plot the actual and predicted time series for an example parcel. Use the same code you used to `plot` the time series from example parcel 195 above to plot the actual time series; then, plot the model-predicted time series for this parcel on top of the actual time series in a different color. You may want to z-score the actual and model-predicted time series for visualization (as the correlation we use to quantify model performance intrinsically z-scores the inputs). Can you visually see a match between the time series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20a73d-c247-464a-a541-eb99a5eb292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted and actual response for example parcel:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4589b2-a416-43a0-8d2d-866769470857",
   "metadata": {},
   "source": [
    "Finally, let's look at a map of the results! Use the `masker`'s `.inverse_transform()` method to convert the parcel-level correlation scores back to into a brain image for visualization. Use `plot_stat_map` to visualize the brain map. I recommend using `vmax=.4`, `threshold=.1`, and consider visualizing both `cut_coords=(-55, -24, 9)` and `cut_coords=(5, -60, 33)` (I also used the `RdYlBu_r` color map)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9186fe04-e46c-43ed-86b9-f854f744c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert masker transform to project onto brain:\n",
    "\n",
    "# Plot encoding performance correlations on brain:\n",
    "\n",
    "# Plot correlations to visualize posterior medial cortex:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f9874f-c1e9-411c-a00e-3964e30cc729",
   "metadata": {},
   "source": [
    "***Run it back!*** Before you proceed to the next section, go back to the cell where we picked one particular model—e.g., `X = X_w2v`. Switch to the other model (`X = X_gpt2`) and re-run the same set of cells up to this point. Take a moment to appreciate the difference between how each model performs separately. Note, however, that running to models separately and comparing their peformance is not a very sophisticated version of model comparison, because the two models will likely be correlated with each other (i.e., the features will be collinear across models). In the following section, we introduce a more advanced model comparison framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85d9ee2-4b41-425c-ac12-f3babb316080",
   "metadata": {},
   "source": [
    "### Banded ridge regression\n",
    "In the previous section, we fit separate models for the word2vec and GPT-2 embeddings. However, these embeddings likely encode a lot of similar information, making it difficult to compare the feature spaces. One way to compare feature spaces is to fit both feature spaces jointly, allowing them both to vie for variance in the brain activity—then evaluate their predictions separately. Our first thought might be to horizontally stack the word2vec and GPT-2 features, then refit the model in the same as the previous section. The problem with this approach is that our encoding model will find a single hyperparameter for both feature spaces. If the feature spaces are qualitatively different from each other, the hyperparameter will likely be unfairly biased toward one feature space. For example, if one feature space is very high-dimensional (i.e. wide) and the other is not, the fitting procedure will likely find a strong penalty term to regularize the wide predictors—but this hyperparameter may overly \"squeeze\" the lower-dimensional predictors. To solve this problem, we'll use Himalaya's implementation of *banded ridge regression* to estimate a separate regularization parameter for each feature space (i.e. band) in the joint predictor matrix ([Nunez-Elizalde et al., 2019](https://doi.org/10.1016/j.neuroimage.2019.04.012); [Dupré la Tour et al., 2022](https://doi.org/10.1016/j.neuroimage.2022.119728)). The first step is to horizontally-stack our predictors for both feature space; we'll use `slice`s to keep track of which columns belong to which feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087a23c-8c01-4079-83d9-91020e8124a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal-stack both embeddings to create joint model\n",
    "X_joint = np.hstack([X_w2v, X_gpt2])\n",
    "\n",
    "# Get the widths of each feature band\n",
    "width_w2v = X_w2v.shape[1]\n",
    "width_gpt2 = X_gpt2.shape[1]\n",
    "\n",
    "# Set up slices for each feature band\n",
    "slice_w2v = slice(0, width_w2v)\n",
    "slice_gpt2 = slice(width_w2v, width_w2v + width_gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f380fa8-20c7-4942-9123-4278d3861581",
   "metadata": {},
   "source": [
    "Print the shape of the joint predictor matrix, as well as both of the slicers, and make sure the numbers make sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3a9cc-9b90-488c-b766-f030b3cfa605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the joint matrix and slices:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566f1761-6fea-49cc-bbda-77d30ad9e62e",
   "metadata": {},
   "source": [
    "We'll set up the usual cross-validation scheme here. We need to take special care to apply the usual transforms—especially the kernelization step—to each set of predictors separately. We'll build a pipeline containing the necessary transforms, then use `himalaya`'s `ColumnKernelizer` apply this pipeline separately to both sets of predictors according to the `slice`s we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c9c61-57dc-4ddf-87f0-00e38af545d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from himalaya.kernel_ridge import Kernelizer, ColumnKernelizer\n",
    "\n",
    "# Split-half outer and inner cross-validation\n",
    "outer_cv = KFold(n_splits=2)\n",
    "inner_cv = KFold(n_splits=5)\n",
    "\n",
    "# Make pipeline with kernelizer for each feature space\n",
    "column_pipeline = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    "    Delayer(delays=[2, 3, 4, 5]),\n",
    "    Kernelizer(kernel=\"linear\"),\n",
    ")\n",
    "\n",
    "# Compile joint column kernelizer\n",
    "column_kernelizer = ColumnKernelizer(\n",
    "    [('word2vec', column_pipeline, slice_w2v),\n",
    "     ('gpt2', column_pipeline, slice_gpt2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339ca57-727b-4461-9690-866d54d36931",
   "metadata": {},
   "source": [
    "We'll use `MultipleKernelRidgeCV` to implement banded ridge regression with hyperparameter search. Since banded ridge regression applies separate penalty terms to each feature space, we need to search over combinations of hyperparameters. This can become very computationally expensive, so we'll use random search to find a good combination of regularization parameters. We initialize the model and link it up to our `ColumnKernelizer` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e907f913-0a10-4fee-a96f-be023adc1fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from himalaya.kernel_ridge import MultipleKernelRidgeCV\n",
    "\n",
    "# Ridge regression with alpha grid and nested CV\n",
    "solver = 'random_search'\n",
    "n_iter = 20\n",
    "alphas = np.logspace(1, 10, 10)\n",
    "solver_params = dict(n_iter=n_iter, alphas=alphas)\n",
    "\n",
    "# Banded ridge regression with column kernelizer\n",
    "banded_ridge = MultipleKernelRidgeCV(kernels=\"precomputed\", solver=solver,\n",
    "                                     solver_params=solver_params, cv=inner_cv)\n",
    "\n",
    "# Chain transfroms and estimator into pipeline\n",
    "pipeline = make_pipeline(column_kernelizer, banded_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a10f30e-654f-4a5e-a47b-d4eb9a9945d2",
   "metadata": {},
   "source": [
    "First, similarly to the previous exercise, we'll fit the joint model, generate a single set of predictions based on the combined feature spaces, and evaluate the quality of these predictions by computing the correlation between actual and predicted time series. This gives us an overall idea of how well the joint model predicts brain activity. Based on the code you used in previous section, inside the outer cross-validation loop, use the `pipeline`'s `.fit()` method to train the model (on the training samples from the model and target) and the `.predict()` method to generate predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb2a20-8263-4b13-9310-62b9b1a00e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through outer folds and estimate model\n",
    "Y_predicted = []\n",
    "for train, test in outer_cv.split(Y_parcels):\n",
    "    \n",
    "    # Fit the pipeline on X and Y training samples:\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Compute predicted response for X test samples and append:\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "# Restack first and second half predictions:\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e9f3a-444d-4626-b537-19a85fac7763",
   "metadata": {},
   "source": [
    "Use `correlation_score` to compute model performance values per parcel like you did in the previous section. Print out the mean and maximum correlation scores along with a histogram across parcels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f01ea59-b35f-45a4-bc10-8f694f245808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions using correlation between predicted and actual time series:\n",
    "\n",
    "# Print mean and maximum encoding performance across parcels:\n",
    "\n",
    "# Plot a histogram of prediction performance values:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e2bc66-74b2-401e-8644-d777ec5d8b5b",
   "metadata": {},
   "source": [
    "Based on your code from the previous section, using the masker's `inverse_transform()` method to convert the parcel-level correlation scores into a brain image and visualize the model performance map using `plot_stat_map` (using the same arguments suggested above). This is not the end of the story, though! Next, we'll look at the relative contributions of each feature space..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b07906-85ea-4384-b180-358ab9123526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert masker transform to project onto brain:\n",
    "\n",
    "# Plot encoding performance correlations on brain:\n",
    "\n",
    "# Plot correlations to visualize posterior medial cortex:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f169d78-ef12-4720-b621-8e56acca94ea",
   "metadata": {},
   "source": [
    "### Comparing feature spaces\n",
    "Previously we estimated the joint model containing both word2vec and GPT-2 embeddings and evaluated the performance of the combined feature spaces. However, the more important question is typically: How do these feature spaces compare to one another? Here, we fit the joint model in the same way as the previous section—but critically we generate predictions separately for each feature space (using `split=True` in the `.predict()` method of the pipeline culminating in `MultipleKernelRidgeCV`). Remember that we generate model-based predictions by multiplying the weight vector estimated from the training set with the predictors from the test set. In this approach, we effectively zero-out weights for the feature-space(s)-of-no-interest to quantify how much one feature space contributes to performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d3b5ab-846a-413f-bdc0-3d0576d06d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through outer folds and estimate model\n",
    "Y_predicted = []\n",
    "for train, test in outer_cv.split(Y_parcels):\n",
    "    \n",
    "    # Fit pipeline with transforms and ridge estimator\n",
    "    pipeline.fit(X_joint[train],\n",
    "                 Y_parcels[train])\n",
    "    \n",
    "    # Compute predicted response\n",
    "    predicted = pipeline.predict(X_joint[test], split=True)\n",
    "    Y_predicted.append(predicted)\n",
    "    \n",
    "# Restack first and second half predictions\n",
    "Y_predicted = np.concatenate(Y_predicted, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3a984c-1a2b-42fe-8496-6f5180d41e6b",
   "metadata": {},
   "source": [
    "Use `correlation_score_split` from `himalaya.scoring` to easily compute correlation scores for the predictions based on each feature space. This will give you separate scores for word2vec and GPT-2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d288b-a3c5-4107-ad67-c3fc6fd84aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation between predicted and actual for split predictions:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba60b551-2698-4e66-bca6-d9f998d3f8ac",
   "metadata": {},
   "source": [
    "Let's look at the results! First, print out the mean and maximum correlation scores for both word2vec and GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3abad3-aa44-4b63-90c7-8e65e3f064ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print mean and maximum correlation scores for word2vec and GPT-2:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3779fc4-3491-48d2-916b-8639423d2c2c",
   "metadata": {},
   "source": [
    "Finally, use the masker's `inverse_transform()` method to convert both sets of results back into two separate model performance maps. Plot these model performance maps using the procedure above (`plot_stat_map`) separately for the word2vec feature band and the GPT-2 feature band. How do the models compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93ec9d-67b2-4d95-a1c8-f2127428ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert masker transform to project word2vec results onto brain:\n",
    "\n",
    "# Plot word2vec encoding performance correlations on brain:\n",
    "\n",
    "# Plot correlations to visualize posterior medial cortex:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3d347-4375-43a9-a51e-8ba7266c4a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert masker transform to project GPT-2 results onto brain:\n",
    "\n",
    "# Plot GPT-2 encoding performance correlations on brain:\n",
    "\n",
    "# Plot correlations to visualize posterior medial cortex:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8916e6cb-83ad-40c6-94ec-f46631a59010",
   "metadata": {},
   "source": [
    "Write a brief reflection (a couple sentences) about what you learned from these analyses. Consider the following points in your response:\n",
    "\n",
    "* Consider which model performs best and in which brain areas—why do you think this model is superior?\n",
    "\n",
    "* Can you think of any ways to even more stringently compare the two models?\n",
    "\n",
    "* How do these encoding model performance maps compare to the contrast maps from Lab 1 and the ISC maps from Lab 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fecbef-b613-4941-8d7b-1a00c47d2935",
   "metadata": {},
   "source": [
    "#### Reflection\n",
    "*Your response here!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5069f5a-66eb-414f-9e80-8e8399bdb0f1",
   "metadata": {},
   "source": [
    "### References\n",
    "* Dupré la Tour, T., Eickenberg, M., Nunez-Elizalde, A. O., & Gallant, J. L. (2022). Feature-space selection with banded ridge regression. *NeuroImage*, *264*, 119728. https://doi.org/10.1016/j.neuroimage.2022.119728\n",
    "\n",
    "* Dupré la Tour, T., Visconti di Oleggio Castello, M., & Gallant, J. L. (2025). The voxelwise encoding model framework: a tutorial introduction to fitting encoding models to fMRI data. *Imaging Neuroscience*, *3*, imag_a_00575. https://doi.org/10.1162/imag_a_00575\n",
    "\n",
    "* Huth, A. G., De Heer, W. A., Griffiths, T. L., Theunissen, F. E., & Gallant, J. L. (2016). Natural speech reveals the semantic maps that tile human cerebral cortex. *Nature*, *532*(7600), 453–458. \n",
    "\n",
    "* Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. *arXiv*. https://doi.org/10.48550/arXiv.1301.3781\n",
    "\n",
    "* Naselaris, T., Kay, K. N., Nishimoto, S., & Gallant, J. L. (2011). Encoding and decoding in fMRI. *NeuroImage*, *56*(2), 400-410. https://doi.org/10.1016/j.neuroimage.2010.07.073\n",
    "\n",
    "* Nastase, S. A., Liu, Y.-F., Hillman, H., Zadbood, A., Hasenfratz, L., Keshavarzian, N., Chen, J., Honey, C. J., Yeshurun, Y., Regev, M., Nguyen, M., Chang, C. H. C., Baldassano, C., Lositsky, O., Simony, E., Chow, M. A., Leong, Y. C., Brooks, P. P., Micciche, E., Choe, G., Goldstein, A., Vanderwal, T., Halchenko, Y. O., Norman, K. A., & Hasson, U. (2021). The \"Narratives\" fMRI dataset for evaluating models of naturalistic language comprehension. *Scientific Data*, *8*, 250. https://doi.org/10.1038/s41597-021-01033-3\n",
    "\n",
    "* Nunez-Elizalde, A. O., Huth, A. G., & Gallant, J. L. (2019). Voxelwise encoding models with non-spherical multivariate normal priors. *NeuroImage*, *197*, 482-492. https://doi.org/10.1016/j.neuroimage.2019.04.012\n",
    "\n",
    "* Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. *OpenAI Blog*. https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "\n",
    "* Schaefer, A., Kong, R., Gordon, E. M., Laumann, T. O., Zuo, X. N., Holmes, A. J., Eickhoff, S. B., & Yeo, B. T. (2018). Local-global parcellation of the human cerebral cortex from intrinsic functional connectivity MRI. *Cerebral Cortex*, *28*(9), 3095–3114. https://doi.org/10.1093/cercor/bhx179\n",
    "\n",
    "* Zada, Z., Goldstein, A. Y., Michelmann, S., Simony, E., Price, A., Hasenfratz, L., Barham, E., Zadbood, A., Doyle, W., Friedman, D., Dugan, P., Melloni, L., Devore, S., Flinker, A., Devinsky, O., Hasson, U.\\*, & Nastase, S. A.\\* (2024). A shared model-based linguistic space for transmitting our thoughts from brain to brain in natural conversations. *Neuron*, *112*(18), 3211–3222. https://doi.org/10.1016/j.neuron.2024.06.025\n",
    "\n",
    "* Zada, Z., Nastase, S. A., Speer, S., Mwilambwe-Tshilobo, L., Tsoi, L., Burns, S., Falk, E., Hasson, U., & Tamir, D. (2025). Linguistic coupling between neural systems for speech production and comprehension during real-time dyadic conversations. *Neuron*, *114*(4), 774–787. https://doi.org/10.1016/j.neuron.2025.11.004"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
